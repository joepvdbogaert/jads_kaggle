{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Experimentation\n",
    "\n",
    "__Note: since there is no predefined RMSE loss function in Keras, this notebooks optimizes the MSE only__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nn_predictors import MultiLayerPerceptron\n",
    "import keras\n",
    "\n",
    "x_train = pd.read_pickle(\"../data/x_train.pkl\") \n",
    "y_train = pd.read_pickle(\"../data/y_train.pkl\") \n",
    "x_test = pd.read_pickle(\"../data/x_test.pkl\")\n",
    "\n",
    "# fill NaNs\n",
    "x_train.fillna(0, inplace=True)\n",
    "y_train.fillna(0, inplace=True)\n",
    "x_test.fillna(0, inplace=True)\n",
    "\n",
    "# Save the dataset ids\n",
    "id_x_train =  x_train['fullVisitorId']\n",
    "id_y_train = y_train['fullVisitorId']\n",
    "id_x_test = x_test['fullVisitorId']\n",
    "\n",
    "# Delete fullVisitor ID -> probably we want to leave it as a OHE feature\n",
    "del x_train['fullVisitorId']\n",
    "del y_train['fullVisitorId']\n",
    "del x_test['fullVisitorId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Some final preparations:\n",
    "- drop some columns\n",
    "- log transform revenue features and the target revenue\n",
    "- normalize the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"People who buy: {} out of {} (proportion: {})\"\n",
    "      .format(int(np.sum(y_train > 0)), int(len(y_train)), float(np.sum(y_train > 0)/len(y_train))))\n",
    "\n",
    "to_drop = ['category__other category', 'category__0']\n",
    "x_train.drop(labels=to_drop, axis=1, inplace=True)\n",
    "x_test.drop(labels=to_drop, axis=1, inplace=True)\n",
    "\n",
    "for col in x_train.columns:\n",
    "    if \"transactionRevenue\" in col:\n",
    "        x_train[col] = np.log(x_train[col] + 1)\n",
    "        x_test[col] = np.log(x_test[col] + 1)\n",
    "    else:\n",
    "        x_train[col] / (x_train[col].max() + 1e-6)\n",
    "        x_test[col] / (x_train[col].max() + 1e-6)\n",
    "\n",
    "y_train = np.log(y_train + 1)\n",
    "assert (x_train.columns == x_test.columns).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple train and evaluate\n",
    "\n",
    "The MLP class can be used to fit a single model and predict / evaluate it. Works mostly like you would expect. Note that it takes a dictionary for the hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron(x_train, y_train, loss='mean_squared_error', nfolds=3, prediction_type=\"regression\")\n",
    "\n",
    "params = {'learning_rate': 0.0001,\n",
    "          'lr_decay': 0.0,\n",
    "          'dropout_rate': 0.1,\n",
    "          'neurons': 32,\n",
    "          'hidden_layers': 4,\n",
    "          'batch_size': 64,\n",
    "          'epochs': 10,\n",
    "          'activation': 1,\n",
    "          'optimizer': 1\n",
    "          }\n",
    "\n",
    "mlp.fit(params, stratified_batches=True)\n",
    "loss = mlp.evaluate(x_train, y_train)\n",
    "print(\"Evaluation loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune with HORD\n",
    "\n",
    "The class has built-in methods to tune hyperparameters using [\"Efficient Hyperparameter Optimization of Deep Learning Algorithms Using Deterministic RBF Surrogates\"](https://ilija139.github.io/pub/aaai-17.pdf) (AAAI-17) by Ilija Ilievski, Taimoor Akhtar, Jiashi Feng, and Christine Annette Shoemaker. \n",
    "\n",
    "[arXiv](https://arxiv.org/abs/1607.08316) -- [PDF](https://ilija139.github.io/pub/aaai-17.pdf) -- [Supplement](https://ilija139.github.io/pub/aaai-17-sup.pdf) -- [Poster](https://ilija139.github.io/pub/aaai-17_poster.pdf)\n",
    "\n",
    "Code is also inspired by the corresponding [GitHub Repository](https://github.com/ilija139/HORD). Hyperparameter Optimization via RBF and Dynamic coordinate search (HORD) is specifically designed to tune Deep Learning models with many parameters and the authors show that it performs well for models with up to 19 hyperparameters.\n",
    "\n",
    "The class implements this functionality with a simple method `.tune_with_HORD(max_evaluations=...)`.\n",
    "Try it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron(x_train, y_train, loss='mean_squared_error', nfolds=5, prediction_type=\"regression\",\n",
    "                           stratify_labels=np.array(y_train > 0, dtype=int))\n",
    "\n",
    "mlp.tune_with_HORD(max_evaluations=100, log=True, log_path=\"../results/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
